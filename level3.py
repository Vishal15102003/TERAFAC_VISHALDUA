# -*- coding: utf-8 -*-
"""Level3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xQ7Ov1WhSdeSPONfrfJ4oZDpjXMlW5D5
"""

!pip install timm -q

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader, random_split
import torchvision.transforms as transforms
import timm
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import random
import os

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

train_tf = transforms.Compose([
    transforms.Resize(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(224, padding=16),
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.406],
        [0.229, 0.224, 0.225]
    )
])

test_tf = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.406],
        [0.229, 0.224, 0.225]
    )
])

dataset = CIFAR10("./data", train=True, download=True, transform=train_tf)
test_set = CIFAR10("./data", train=False, download=True, transform=test_tf)

train_set, val_set = random_split(
    dataset, [45000, 5000],
    generator=torch.Generator().manual_seed(42)
)

test_set = torch.utils.data.Subset(test_set, range(5000))

train_loader = DataLoader(train_set, batch_size=128, shuffle=True)
val_loader   = DataLoader(val_set, batch_size=128)
test_loader  = DataLoader(test_set, batch_size=128)

class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = x.mean(dim=[2, 3])
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class ConvNeXtAttention(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.backbone = timm.create_model(
            "convnext_tiny",
            pretrained=True,
            features_only=True
        )
        self.attn = SEBlock(768)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.head = nn.Linear(768, num_classes)

    def forward(self, x):
        feat = self.backbone(x)[-1]
        feat = self.attn(feat)
        feat = self.pool(feat).flatten(1)
        return self.head(feat)

model = ConvNeXtAttention().to(DEVICE)
print("Parameters:", sum(p.numel() for p in model.parameters()) // 1_000_000, "M")

criterion = nn.CrossEntropyLoss(label_smoothing=0.05)

optimizer = optim.AdamW(
    model.parameters(),
    lr=3e-4,
    weight_decay=0.05
)

scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

def run_epoch(model, loader, train=True):
    model.train() if train else model.eval()
    correct, total, loss_sum = 0, 0, 0

    with torch.set_grad_enabled(train):
        for x, y in tqdm(loader, leave=False):
            x, y = x.to(DEVICE), y.to(DEVICE)

            if train:
                optimizer.zero_grad()

            out = model(x)
            loss = criterion(out, y)

            if train:
                loss.backward()
                optimizer.step()

            loss_sum += loss.item()
            correct += (out.argmax(1) == y).sum().item()
            total += y.size(0)

    return loss_sum / len(loader), correct / total

EPOCHS = 15
best_val = 0
history = {"train": [], "val": []}

for ep in range(EPOCHS):
    _, tr_acc = run_epoch(model, train_loader, True)
    _, va_acc = run_epoch(model, val_loader, False)
    scheduler.step()

    history["train"].append(tr_acc)
    history["val"].append(va_acc)

    print(f"Epoch {ep+1}: Train={tr_acc:.4f}, Val={va_acc:.4f}")

    if va_acc > best_val:
        best_val = va_acc
        torch.save(model.state_dict(), "level3_best.pth")

model.load_state_dict(torch.load("level3_best.pth"))
_, test_acc = run_epoch(model, test_loader, False)

print("\nLEVEL-3 TEST ACCURACY:", round(test_acc * 100, 2), "%")

classes = ['airplane','automobile','bird','cat','deer',
           'dog','frog','horse','ship','truck']

class_correct = [0]*10
class_total = [0]*10

model.eval()
with torch.no_grad():
    for x,y in test_loader:
        x,y = x.to(DEVICE), y.to(DEVICE)
        out = model(x)
        preds = out.argmax(1)
        for i in range(len(y)):
            class_total[y[i]] += 1
            class_correct[y[i]] += (preds[i]==y[i]).item()

for i in range(10):
    print(f"{classes[i]:12s}: {100*class_correct[i]/class_total[i]:.2f}%")

plt.plot([a*100 for a in history["train"]], label="Train")
plt.plot([a*100 for a in history["val"]], label="Val")
plt.axhline(y=91, linestyle="--", color="green", label="Target")
plt.xlabel("Epoch")
plt.ylabel("Accuracy (%)")
plt.title("Level-3 Training Curve")
plt.legend()
plt.grid(alpha=0.3)
plt.show()